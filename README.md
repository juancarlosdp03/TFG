El archivo [`chompagente.ipynb`](./chompagente.ipynb) contiene el código usado para entrenar a dos agentes en el Chomp usando Monte Carlo. También se encuentra en él la función que te devuelve un tabla con las acciones que elige el jugador 1 desde los estados que le pidamos siguiendo la política aprendida tras el entrenamiento, y también muestra el estado al que llega tras realizar cada acción. Para ver otro ejemplo basta con cambiar la lista "states_to_check". El gráfico de la tasa de victorias es diferente porque, como se explica en el trabajo, las estrategias en casos particulares se hacen en un tablero 4x4, no en uno 4x6, que es el que se usó para ver la tasa de victorias. Cambiando la línea "action = policy1.get(state)" por "action = policy2.get(state)" se obtiene lo mismo pero para el jugador 2.
El archivo [`chomprandom.ipynb`](./chomprandom.ipynb) contiene el código usado para entrenar a un agente en el Chomp contra un rival que hace movimientos al azar usando Monte Carlo. También se visualiza el gráfico de la tasa de victorias que aparece en el trabajo.
El archivo [`nim_vs_agente.py`](./nim_vs_agente.py) contiene el código usado para entrenar a un agente en el NIM contra el jugador que conoce la solución matemática usando Monte Carlo. Posteriormente guarda en [`modelo_nim.pkl`](./modelo_nim.pkl) la política aprendida, e inicializa una partida con tablero aleatorio para que un humano pueda jugar contra el agente entrenado. Al ejecutar el código se podrá jugar una partida directamente.
El archivo [`nimmcjo.ipynb`](./nimmcjo.ipynb) entrena al agente igual que en [`nim_vs_agente.py`](./nim_vs_agente.py), y muestra la gráfica de tasa de victorias y movimientos óptimos.
El archivo [`nimmcrandom.ipynb`](./nimmcrandom.ipynb) entrena al agente igual que en [`nimmcjo.ipynb`](./nimmcjo.ipynb) y muestra los mismos gráficos, pero esta vez el rival hace movimientos aleatorios. 
El archivo [`nimmcagente.ipynb`](./nimmcagente.ipynb) entrena a dos agentes a la vez y muestra las gráficas de ambos.
El resto de archivos hace lo mismo, cambiando mc por sarsa cuando el algoritmo usado en el entrenamiento sea SARSA, y por ql o qlearning cuando el algoritmo usado es Q-learning.
